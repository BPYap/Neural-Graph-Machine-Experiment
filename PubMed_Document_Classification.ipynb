{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing graph links...\n",
      "Loading and processing graph nodes...\n",
      "('Number of links:', 44338)\n",
      "('Number of nodes:', 19717)\n",
      "('Number of nodes belong to Class 1', 4103)\n",
      "('Number of nodes belong to Class 2', 7875)\n",
      "('Number of nodes belong to Class 3', 7739)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "DATA_DIR = \"data/PubMed/\"\n",
    "EDGE_PATH = DATA_DIR + \"Pubmed-Diabetes.DIRECTED.cites.tab\"\n",
    "NODE_PATH = DATA_DIR + \"Pubmed-Diabetes.NODE.paper.tab\"\n",
    "TF_IDF_DIM = 500\n",
    "\n",
    "# Load and process graph links\n",
    "print(\"Loading and processing graph links...\")\n",
    "node_pairs = set()\n",
    "with open(EDGE_PATH, 'r') as f:\n",
    "    next(f)  # skip header\n",
    "    next(f)  # skip header\n",
    "    for line in f:\n",
    "        columns = line.split()\n",
    "        src = int(columns[1][6:])\n",
    "        dest = int(columns[3].strip()[6:])\n",
    "        node_pairs.add((src, dest))\n",
    "        \n",
    "# Load and process graph nodes\n",
    "print(\"Loading and processing graph nodes...\")\n",
    "node2vec = OrderedDict()\n",
    "node2label = dict()\n",
    "class_1 = list()\n",
    "class_2 = list()\n",
    "class_3 = list()\n",
    "with open(NODE_PATH, 'r') as f:\n",
    "    next(f)  # skip header\n",
    "    vocabs = [e.split(':')[1] for e in next(f).split()[1:]]\n",
    "    for line in f:\n",
    "        columns = line.split()\n",
    "        node = int(columns[0])\n",
    "        label = int(columns[1][-1])\n",
    "        tf_idf_vec = [0.0] * TF_IDF_DIM\n",
    "\n",
    "        for e in columns[2:-1]:\n",
    "            word, value = e.split('=')\n",
    "            tf_idf_vec[vocabs.index(word)] = float(value)\n",
    "\n",
    "        node2vec[node] = tf_idf_vec\n",
    "        node2label[node] = label - 1\n",
    "        if label == 1:\n",
    "            class_1.append(node)\n",
    "        elif label == 2:\n",
    "            class_2.append(node)\n",
    "        elif label == 3:\n",
    "            class_3.append(node)\n",
    "\n",
    "# Debug statistics\n",
    "print(\"Number of links:\", len(node_pairs))\n",
    "assert len(node2vec) == (len(class_1) + len(class_2) + len(class_3))\n",
    "print(\"Number of nodes:\", len(node2vec))\n",
    "print(\"Number of nodes belong to Class 1\", len(class_1))\n",
    "print(\"Number of nodes belong to Class 2\", len(class_2))\n",
    "print(\"Number of nodes belong to Class 3\", len(class_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"model/\"\n",
    "TEST_SIZE = 1000\n",
    "SEED_NODES = 20\n",
    "NUM_CATEGORIES = 3\n",
    "\n",
    "ALPHA = 0.2\n",
    "HIDDEN_1_DIM = 250\n",
    "HIDDEN_2_DIM = 100\n",
    "\n",
    "NUM_EPOCH = 12\n",
    "BATCH_SIZE = 12\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important variables from previous cells: node_pairs, class_1, class_2, class_3\n",
    "test_nodes = class_1[-TEST_SIZE:] + class_2[-TEST_SIZE:] + class_3[-TEST_SIZE:]\n",
    "train_node_pairs = []\n",
    "for src, dest in node_pairs:\n",
    "    if not (src in test_nodes or dest in test_nodes):\n",
    "        train_node_pairs.append((src, dest))\n",
    "\n",
    "seed_nodes = class_1[:SEED_NODES] + class_2[:SEED_NODES] + class_3[:SEED_NODES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class NGM_FFNN(nn.Module):\n",
    "    def __init__(self, alpha, input_dim, hidden1_dim, hidden2_dim, output_dim, device=torch.device('cpu')):\n",
    "        super(NGM_FFNN, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.hidden2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.output = nn.Linear(hidden2_dim, output_dim)\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def save(self, output_dir, model_name):\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(self.state_dict(), output_dir + model_name + \".pt\")\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "    def load(self, output_dir, model_name):\n",
    "        print(\"Loading model...\")\n",
    "        self.load_state_dict(torch.load(output_dir + model_name + \".pt\"))\n",
    "        print(\"Model loaded.\")\n",
    "        \n",
    "    def forward(self, tf_idf_vec):\n",
    "        # First feed-forward layer\n",
    "        hidden1 = F.relu(self.hidden1(tf_idf_vec))\n",
    "\n",
    "        # Second feed-forward layer\n",
    "        hidden2 = F.relu(self.hidden2(hidden1))\n",
    "\n",
    "        # Output layer\n",
    "        return F.log_softmax(self.output(hidden2), -1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.hidden1.reset_parameters()\n",
    "        self.hidden2.reset_parameters()\n",
    "        self.output.reset_parameters()\n",
    "    \n",
    "    def get_last_hidden(self, tf_idf_vec):\n",
    "        # First feed-forward layer\n",
    "        hidden1 = F.relu(self.hidden1(tf_idf_vec))\n",
    "\n",
    "        # Second feed-forward layer\n",
    "        return F.relu(self.hidden2(hidden1))\n",
    "    \n",
    "    def train_(self, seed_nodes, train_node_pairs, node2vec, node2label, \n",
    "               num_epoch, batch_size, learning_rate):\n",
    "        print(\"Training...\")\n",
    "        self.train()\n",
    "\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        node2neighbors = defaultdict(list)\n",
    "        for src, dest in train_node_pairs:\n",
    "            node2neighbors[src].append(dest)\n",
    "            node2neighbors[dest].append(src)\n",
    "            \n",
    "        labeled_nodes = dict()\n",
    "        for node in seed_nodes:\n",
    "            labeled_nodes[node] = node2label[node]\n",
    "\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            print(\"=\" * 80)\n",
    "            print \"Generation: {} (with {} labeled nodes)\".format(iteration, len(labeled_nodes))\n",
    "            iteration += 1\n",
    "\n",
    "            for e in range(num_epoch):\n",
    "                train_node_pairs_cpy =  train_node_pairs[:]\n",
    "                total_loss = 0\n",
    "                while train_node_pairs_cpy:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = torch.tensor(0, dtype=torch.float32, device=self.device)\n",
    "                    label_label_loss = defaultdict(list)\n",
    "                    label_unlabel_loss = defaultdict(list)\n",
    "                    \n",
    "                    try:\n",
    "                        batch = random.sample(train_node_pairs_cpy, batch_size)\n",
    "                    except ValueError:\n",
    "                        break\n",
    "                        \n",
    "                    for src, dest in batch:\n",
    "                        train_node_pairs_cpy.remove((src, dest))\n",
    "\n",
    "                        if src in labeled_nodes:\n",
    "                            tf_idf_vector = torch.tensor(node2vec[src], device=self.device)\n",
    "                            targets = torch.tensor([labeled_nodes[src]], device=self.device)\n",
    "                            lg_softmax = self.forward(tf_idf_vector)\n",
    "                            nll_loss = loss_function(lg_softmax.view(1, -1), targets)\n",
    "                            # NLL loss from classification\n",
    "                            if dest in labeled_nodes:\n",
    "                                label_label_loss[src].append(nll_loss)\n",
    "                            else:\n",
    "                                label_unlabel_loss[src].append(nll_loss)\n",
    "                            \n",
    "                        if dest in labeled_nodes:\n",
    "                            tf_idf_vector = torch.tensor(node2vec[dest], device=self.device)\n",
    "                            targets = torch.tensor([labeled_nodes[dest]], device=self.device)\n",
    "                            lg_softmax = self.forward(tf_idf_vector)\n",
    "                            nll_loss = loss_function(lg_softmax.view(1, -1), targets)\n",
    "                            # NLL loss from classification\n",
    "                            label_label_loss[dest].append(nll_loss)\n",
    "                    \n",
    "                        if self.alpha != 0:\n",
    "                            if ((src in labeled_nodes and dest in labeled_nodes) or \n",
    "                              (src in labeled_nodes and dest not in labeled_nodes) or \n",
    "                              (src not in labeled_nodes and dest not in labeled_nodes)):\n",
    "                                # L2 loss from labeled-labeled/labeled-unlabeled/\n",
    "                                # unlabeled-unlabeled edges\n",
    "                                src_hidden = self.get_last_hidden(\n",
    "                                    torch.tensor(node2vec[src], device=self.device))\n",
    "                                dest_hidden = self.get_last_hidden(\n",
    "                                    torch.tensor(node2vec[dest], device=self.device))\n",
    "                                loss += self.alpha * torch.dist(src_hidden, dest_hidden)\n",
    "\n",
    "                    for incident in label_label_loss:\n",
    "                        temp = label_label_loss[incident]\n",
    "                        loss += torch.stack(temp).sum() / len(temp)\n",
    "                    \n",
    "                    for incident in label_unlabel_loss:\n",
    "                        temp = label_unlabel_loss[incident]\n",
    "                        loss += torch.stack(temp).sum() / len(temp)\n",
    "                        \n",
    "                    if loss.item() != 0:\n",
    "                        assert not torch.isnan(loss)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        total_loss += loss.item()\n",
    "                        del loss\n",
    "\n",
    "                avg_loss = total_loss / len(labeled_nodes)\n",
    "                print(\"Epoch: {} Loss: {} (avg: {})\".format(e + 1, total_loss, avg_loss))\n",
    "\n",
    "            # Assign label to immediate neighbors\n",
    "            label_changed = False\n",
    "            for node in list(labeled_nodes.keys()):\n",
    "                for neighbor in node2neighbors[node]:\n",
    "                    label = labeled_nodes[node]\n",
    "                    if neighbor not in labeled_nodes:\n",
    "#                         prediction = self.predict(\n",
    "#                             torch.tensor(node2vec[neighbor], device=self.device))\n",
    "                        labeled_nodes[neighbor] = label\n",
    "                        label_changed = True\n",
    "            if not label_changed:\n",
    "                break\n",
    "#             else:\n",
    "#                 self.reset_parameters()\n",
    "\n",
    "    def predict(self, tf_idf_vec):\n",
    "        return torch.argmax(self.forward(tf_idf_vec)).item()\n",
    "        \n",
    "    def evaluate(self, test_nodes, node2vec, node2label):\n",
    "        self.eval()\n",
    "\n",
    "        correct_count = 0\n",
    "        for node in test_nodes:\n",
    "            predicted = self.predict(torch.tensor(node2vec[node], device=self.device))\n",
    "            if predicted == node2label[node]:\n",
    "                correct_count += 1\n",
    "\n",
    "        return float(correct_count) / len(test_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "================================================================================\n",
      "Generation: 1 (with 60 labeled nodes)\n",
      "Epoch: 1 Loss: 265.468328595 (avg: 4.42447214325)\n",
      "Epoch: 2 Loss: 265.283905506 (avg: 4.4213984251)\n",
      "Epoch: 3 Loss: 263.944190741 (avg: 4.39906984568)\n",
      "Epoch: 4 Loss: 264.928966284 (avg: 4.4154827714)\n",
      "Epoch: 5 Loss: 263.604874253 (avg: 4.39341457089)\n",
      "Epoch: 6 Loss: 262.295018077 (avg: 4.37158363461)\n",
      "Epoch: 7 Loss: 264.429841042 (avg: 4.40716401736)\n",
      "Epoch: 8 Loss: 263.191653252 (avg: 4.38652755419)\n",
      "Epoch: 9 Loss: 264.103588104 (avg: 4.4017264684)\n",
      "Epoch: 10 Loss: 263.947010994 (avg: 4.3991168499)\n",
      "Epoch: 11 Loss: 262.664132357 (avg: 4.37773553928)\n",
      "Epoch: 12 Loss: 262.519589663 (avg: 4.37532649438)\n",
      "================================================================================\n",
      "Generation: 2 (with 293 labeled nodes)\n",
      "Epoch: 1 Loss: 3583.54682088 (avg: 12.2305352248)\n",
      "Epoch: 2 Loss: 3531.30918151 (avg: 12.0522497663)\n",
      "Epoch: 3 Loss: 3479.75781226 (avg: 11.8763065265)\n",
      "Epoch: 4 Loss: 3444.04305166 (avg: 11.7544131456)\n",
      "Epoch: 5 Loss: 3418.04156131 (avg: 11.6656708577)\n",
      "Epoch: 6 Loss: 3387.50046641 (avg: 11.5614350389)\n",
      "Epoch: 7 Loss: 3379.74088341 (avg: 11.5349518205)\n",
      "Epoch: 8 Loss: 3375.04609191 (avg: 11.5189286413)\n",
      "Epoch: 9 Loss: 3356.65228164 (avg: 11.4561511319)\n",
      "Epoch: 10 Loss: 3353.63772255 (avg: 11.4458625343)\n",
      "Epoch: 11 Loss: 3340.1502403 (avg: 11.3998301717)\n",
      "Epoch: 12 Loss: 3331.1860956 (avg: 11.3692358211)\n",
      "================================================================================\n",
      "Generation: 3 (with 2226 labeled nodes)\n",
      "Epoch: 1 Loss: 19025.4404513 (avg: 8.54691844174)\n",
      "Epoch: 2 Loss: 18636.0414672 (avg: 8.37198628358)\n",
      "Epoch: 3 Loss: 18508.3466219 (avg: 8.31462112393)\n",
      "Epoch: 4 Loss: 18448.895417 (avg: 8.28791348471)\n",
      "Epoch: 5 Loss: 18434.2881048 (avg: 8.28135134988)\n",
      "Epoch: 6 Loss: 18431.4535325 (avg: 8.28007795711)\n",
      "Epoch: 7 Loss: 18405.8588535 (avg: 8.26857989823)\n",
      "Epoch: 8 Loss: 18411.2505624 (avg: 8.27100204958)\n",
      "Epoch: 9 Loss: 18404.8500549 (avg: 8.26812670931)\n",
      "Epoch: 10 Loss: 18386.8737761 (avg: 8.26005111234)\n",
      "Epoch: 11 Loss: 18365.4809589 (avg: 8.25044068236)\n",
      "Epoch: 12 Loss: 18386.6278009 (avg: 8.25994061138)\n",
      "================================================================================\n",
      "Generation: 4 (with 7412 labeled nodes)\n",
      "Epoch: 1 Loss: 43039.5240593 (avg: 5.80673557195)\n",
      "Epoch: 2 Loss: 42961.3414621 (avg: 5.79618746116)\n",
      "Epoch: 3 Loss: 42873.317307 (avg: 5.78431156328)\n",
      "Epoch: 4 Loss: 42731.9780607 (avg: 5.76524258779)\n",
      "Epoch: 5 Loss: 42543.0226803 (avg: 5.7397494172)\n",
      "Epoch: 6 Loss: 42244.1307249 (avg: 5.69942400498)\n",
      "Epoch: 7 Loss: 41776.5246105 (avg: 5.63633629392)\n",
      "Epoch: 8 Loss: 41016.7740107 (avg: 5.53383351466)\n",
      "Epoch: 9 Loss: 39720.0456829 (avg: 5.35888365932)\n",
      "Epoch: 10 Loss: 37769.8536983 (avg: 5.09577087132)\n",
      "Epoch: 11 Loss: 35324.8493662 (avg: 4.76589980656)\n",
      "Epoch: 12 Loss: 32827.2542398 (avg: 4.42893338367)\n",
      "================================================================================\n",
      "Generation: 5 (with 12678 labeled nodes)\n",
      "Epoch: 1 Loss: 41566.0002489 (avg: 3.27859285762)\n",
      "Epoch: 2 Loss: 39358.196352 (avg: 3.10444836346)\n",
      "Epoch: 3 Loss: 37664.0417578 (avg: 2.97081887978)\n",
      "Epoch: 4 Loss: 36337.8358684 (avg: 2.86621201044)\n",
      "Epoch: 5 Loss: 35291.7548537 (avg: 2.78370049327)\n",
      "Epoch: 6 Loss: 34516.5781841 (avg: 2.72255704245)\n",
      "Epoch: 7 Loss: 33847.1515214 (avg: 2.66975481318)\n",
      "Epoch: 8 Loss: 33390.4943221 (avg: 2.63373515713)\n",
      "Epoch: 9 Loss: 32990.8611026 (avg: 2.60221336982)\n",
      "Epoch: 10 Loss: 32664.898967 (avg: 2.57650252146)\n",
      "Epoch: 11 Loss: 32366.3133082 (avg: 2.55295104182)\n",
      "Epoch: 12 Loss: 32122.4948335 (avg: 2.53371942211)\n",
      "================================================================================\n",
      "Generation: 6 (with 14632 labeled nodes)\n",
      "Epoch: 1 Loss: 34970.5865629 (avg: 2.39000728286)\n",
      "Epoch: 2 Loss: 34762.9855933 (avg: 2.37581913568)\n",
      "Epoch: 3 Loss: 34526.79159 (avg: 2.35967684459)\n",
      "Epoch: 4 Loss: 34326.4977357 (avg: 2.3459880902)\n",
      "Epoch: 5 Loss: 34087.3941581 (avg: 2.32964694902)\n",
      "Epoch: 6 Loss: 33845.9724734 (avg: 2.31314738063)\n",
      "Epoch: 7 Loss: 33557.246146 (avg: 2.29341485415)\n",
      "Epoch: 8 Loss: 33305.1656752 (avg: 2.27618682854)\n",
      "Epoch: 9 Loss: 32989.3021274 (avg: 2.25459965332)\n",
      "Epoch: 10 Loss: 32626.4659815 (avg: 2.22980221306)\n",
      "Epoch: 11 Loss: 32217.3940849 (avg: 2.20184486638)\n",
      "Epoch: 12 Loss: 31798.6794463 (avg: 2.17322850235)\n",
      "================================================================================\n",
      "Generation: 7 (with 15079 labeled nodes)\n",
      "Epoch: 1 Loss: 32034.3115032 (avg: 2.1244320912)\n",
      "Epoch: 2 Loss: 31528.0275444 (avg: 2.09085665789)\n",
      "Epoch: 3 Loss: 30999.0431434 (avg: 2.0557757904)\n",
      "Epoch: 4 Loss: 30494.4102675 (avg: 2.02230985261)\n",
      "Epoch: 5 Loss: 30082.7204736 (avg: 1.99500765791)\n",
      "Epoch: 6 Loss: 29657.6348768 (avg: 1.96681708846)\n",
      "Epoch: 7 Loss: 29305.3507887 (avg: 1.94345452541)\n",
      "Epoch: 8 Loss: 29050.8793483 (avg: 1.92657864237)\n",
      "Epoch: 9 Loss: 28793.3007559 (avg: 1.9094967011)\n",
      "Epoch: 10 Loss: 28577.3358684 (avg: 1.89517447234)\n",
      "Epoch: 11 Loss: 28405.3671398 (avg: 1.88376995423)\n",
      "Epoch: 12 Loss: 28225.1695284 (avg: 1.87181971804)\n",
      "================================================================================\n",
      "Generation: 8 (with 15191 labeled nodes)\n",
      "Epoch: 1 Loss: 28202.5604331 (avg: 1.85653086914)\n",
      "Epoch: 2 Loss: 28087.8535532 (avg: 1.84897989291)\n",
      "Epoch: 3 Loss: 27943.8773235 (avg: 1.83950216072)\n",
      "Epoch: 4 Loss: 27835.0489181 (avg: 1.83233815536)\n",
      "Epoch: 5 Loss: 27706.5465965 (avg: 1.82387904658)\n",
      "Epoch: 6 Loss: 27616.6723397 (avg: 1.81796276346)\n",
      "Epoch: 7 Loss: 27489.3163861 (avg: 1.8095791183)\n",
      "Epoch: 8 Loss: 27403.8198953 (avg: 1.80395101674)\n",
      "Epoch: 9 Loss: 27327.5232387 (avg: 1.79892852601)\n",
      "Epoch: 10 Loss: 27218.8885677 (avg: 1.79177727389)\n",
      "Epoch: 11 Loss: 27155.8362403 (avg: 1.78762663684)\n",
      "Epoch: 12 Loss: 27076.2683587 (avg: 1.78238880645)\n",
      "================================================================================\n",
      "Generation: 9 (with 15231 labeled nodes)\n",
      "Epoch: 1 Loss: 27070.0410763 (avg: 1.77729900048)\n",
      "Epoch: 2 Loss: 26995.174894 (avg: 1.77238361854)\n",
      "Epoch: 3 Loss: 26906.3088958 (avg: 1.7665490707)\n",
      "Epoch: 4 Loss: 26839.8897719 (avg: 1.7621882852)\n",
      "Epoch: 5 Loss: 26802.0316085 (avg: 1.75970268587)\n",
      "Epoch: 6 Loss: 26726.6771382 (avg: 1.75475524511)\n",
      "Epoch: 7 Loss: 26648.856348 (avg: 1.7496458767)\n",
      "Epoch: 8 Loss: 26577.6367639 (avg: 1.74496991425)\n",
      "Epoch: 9 Loss: 26542.4300451 (avg: 1.74265839703)\n",
      "Epoch: 10 Loss: 26478.0393444 (avg: 1.73843078881)\n",
      "Epoch: 11 Loss: 26431.4148881 (avg: 1.73536963352)\n",
      "Epoch: 12 Loss: 26355.348787 (avg: 1.73037547022)\n",
      "================================================================================\n",
      "Generation: 10 (with 15245 labeled nodes)\n",
      "Epoch: 1 Loss: 26293.0756644 (avg: 1.72470158507)\n",
      "Epoch: 2 Loss: 26253.8886364 (avg: 1.72213110111)\n",
      "Epoch: 3 Loss: 26214.5567747 (avg: 1.71955111674)\n",
      "Epoch: 4 Loss: 26144.0712928 (avg: 1.71492760202)\n",
      "Epoch: 5 Loss: 26085.8387216 (avg: 1.71110782038)\n",
      "Epoch: 6 Loss: 26047.4566367 (avg: 1.70859013688)\n",
      "Epoch: 7 Loss: 26003.0222156 (avg: 1.70567544871)\n",
      "Epoch: 8 Loss: 25948.3530493 (avg: 1.7020894096)\n",
      "Epoch: 9 Loss: 25904.8860192 (avg: 1.69923817771)\n",
      "Epoch: 10 Loss: 25868.5103244 (avg: 1.69685210393)\n",
      "Epoch: 11 Loss: 25793.0308928 (avg: 1.6919010097)\n",
      "Epoch: 12 Loss: 25751.3371311 (avg: 1.68916609584)\n",
      "================================================================================\n",
      "Generation: 11 (with 15246 labeled nodes)\n",
      "Epoch: 1 Loss: 25681.1337552 (avg: 1.68445059394)\n",
      "Epoch: 2 Loss: 25657.5919851 (avg: 1.68290646629)\n",
      "Epoch: 3 Loss: 25581.0023749 (avg: 1.67788287911)\n",
      "Epoch: 4 Loss: 25538.5725666 (avg: 1.67509986663)\n",
      "Epoch: 5 Loss: 25501.8055136 (avg: 1.67268827979)\n",
      "Epoch: 6 Loss: 25433.9599419 (avg: 1.66823822261)\n",
      "Epoch: 7 Loss: 25386.1558675 (avg: 1.66510270677)\n",
      "Epoch: 8 Loss: 25327.1463842 (avg: 1.66123221725)\n",
      "Epoch: 9 Loss: 25304.9611398 (avg: 1.65977706545)\n",
      "Epoch: 10 Loss: 25221.472187 (avg: 1.65430094366)\n",
      "Epoch: 11 Loss: 25182.6509484 (avg: 1.65175462078)\n",
      "Epoch: 12 Loss: 25128.8189417 (avg: 1.64822372699)\n"
     ]
    }
   ],
   "source": [
    "# Important variable from previous cells: node_pairs, node2vec, node2label, seed_nodes, train_node_pairs, test_nodes\n",
    "from datetime import datetime\n",
    "baseline_model = NGM_FFNN(0, TF_IDF_DIM, HIDDEN_1_DIM, HIDDEN_2_DIM, NUM_CATEGORIES)\n",
    "start = datetime.now()\n",
    "baseline_model.train_(seed_nodes, train_node_pairs, node2vec, node2label, NUM_EPOCH, BATCH_SIZE, LEARNING_RATE)\n",
    "baseline_time = (datetime.now()-start).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural graph machine feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "================================================================================\n",
      "Generation: 1 (with 60 labeled nodes)\n",
      "Epoch: 1 Loss: 456.32806924 (avg: 7.60546782066)\n",
      "Epoch: 2 Loss: 441.315778628 (avg: 7.35526297713)\n",
      "Epoch: 3 Loss: 430.109133132 (avg: 7.1684855522)\n",
      "Epoch: 4 Loss: 424.046260335 (avg: 7.06743767224)\n",
      "Epoch: 5 Loss: 419.131263293 (avg: 6.98552105489)\n",
      "Epoch: 6 Loss: 410.430202842 (avg: 6.8405033807)\n",
      "Epoch: 7 Loss: 406.970913898 (avg: 6.78284856497)\n",
      "Epoch: 8 Loss: 402.675507873 (avg: 6.71125846455)\n",
      "Epoch: 9 Loss: 397.419615209 (avg: 6.62366025349)\n",
      "Epoch: 10 Loss: 391.325186882 (avg: 6.52208644804)\n",
      "Epoch: 11 Loss: 385.212630328 (avg: 6.42021050546)\n",
      "Epoch: 12 Loss: 385.697685815 (avg: 6.42829476359)\n",
      "================================================================================\n",
      "Generation: 2 (with 293 labeled nodes)\n",
      "Epoch: 1 Loss: 3662.01440263 (avg: 12.4983426711)\n",
      "Epoch: 2 Loss: 3613.83835028 (avg: 12.3339192842)\n",
      "Epoch: 3 Loss: 3569.20593877 (avg: 12.1815902347)\n",
      "Epoch: 4 Loss: 3543.33056805 (avg: 12.0932783892)\n",
      "Epoch: 5 Loss: 3508.97634298 (avg: 11.9760284743)\n",
      "Epoch: 6 Loss: 3499.08436466 (avg: 11.9422674562)\n",
      "Epoch: 7 Loss: 3468.3566532 (avg: 11.8373947208)\n",
      "Epoch: 8 Loss: 3462.74703785 (avg: 11.8182492759)\n",
      "Epoch: 9 Loss: 3456.09103649 (avg: 11.7955325478)\n",
      "Epoch: 10 Loss: 3442.17785222 (avg: 11.7480472772)\n",
      "Epoch: 11 Loss: 3435.9068291 (avg: 11.7266444679)\n",
      "Epoch: 12 Loss: 3430.28844079 (avg: 11.7074690812)\n",
      "================================================================================\n",
      "Generation: 3 (with 2226 labeled nodes)\n",
      "Epoch: 1 Loss: 19214.5091024 (avg: 8.63185494266)\n",
      "Epoch: 2 Loss: 18780.4819376 (avg: 8.43687418582)\n",
      "Epoch: 3 Loss: 18647.0877707 (avg: 8.37694868407)\n",
      "Epoch: 4 Loss: 18568.6419364 (avg: 8.34170796782)\n",
      "Epoch: 5 Loss: 18553.4837319 (avg: 8.33489835214)\n",
      "Epoch: 6 Loss: 18522.5876571 (avg: 8.32101871388)\n",
      "Epoch: 7 Loss: 18532.8352264 (avg: 8.32562229399)\n",
      "Epoch: 8 Loss: 18521.6585693 (avg: 8.32060133393)\n",
      "Epoch: 9 Loss: 18524.2834842 (avg: 8.32178054095)\n",
      "Epoch: 10 Loss: 18519.807568 (avg: 8.31976979696)\n",
      "Epoch: 11 Loss: 18524.1161509 (avg: 8.32170536877)\n",
      "Epoch: 12 Loss: 18513.6948791 (avg: 8.31702375523)\n",
      "================================================================================\n",
      "Generation: 4 (with 7412 labeled nodes)\n",
      "Epoch: 1 Loss: 43280.3967996 (avg: 5.83923324333)\n",
      "Epoch: 2 Loss: 43284.4525933 (avg: 5.83978043623)\n",
      "Epoch: 3 Loss: 43278.005053 (avg: 5.83891055762)\n",
      "Epoch: 4 Loss: 43266.5146499 (avg: 5.83736031434)\n",
      "Epoch: 5 Loss: 43269.2700319 (avg: 5.83773206043)\n",
      "Epoch: 6 Loss: 43261.7661176 (avg: 5.83671965968)\n",
      "Epoch: 7 Loss: 43281.6482205 (avg: 5.83940208048)\n",
      "Epoch: 8 Loss: 43245.3453641 (avg: 5.83450423153)\n",
      "Epoch: 9 Loss: 43252.7682977 (avg: 5.83550570665)\n",
      "Epoch: 10 Loss: 43226.0269504 (avg: 5.83189786162)\n",
      "Epoch: 11 Loss: 43214.5587153 (avg: 5.83035060919)\n",
      "Epoch: 12 Loss: 43234.795681 (avg: 5.83308090677)\n",
      "================================================================================\n",
      "Generation: 5 (with 12678 labeled nodes)\n",
      "Epoch: 1 Loss: 56093.8617201 (avg: 4.42450400064)\n",
      "Epoch: 2 Loss: 56056.2095184 (avg: 4.42153411567)\n",
      "Epoch: 3 Loss: 55991.4058065 (avg: 4.41642260661)\n",
      "Epoch: 4 Loss: 55941.1175623 (avg: 4.4124560311)\n",
      "Epoch: 5 Loss: 55892.8857765 (avg: 4.40865166245)\n",
      "Epoch: 6 Loss: 55800.7720823 (avg: 4.40138602953)\n",
      "Epoch: 7 Loss: 55689.2339649 (avg: 4.39258826037)\n",
      "Epoch: 8 Loss: 55487.6055908 (avg: 4.37668446055)\n",
      "Epoch: 9 Loss: 55188.7547808 (avg: 4.35311206663)\n",
      "Epoch: 10 Loss: 54676.4267607 (avg: 4.3127012747)\n",
      "Epoch: 11 Loss: 53725.9284925 (avg: 4.23772901818)\n",
      "Epoch: 12 Loss: 52174.6325684 (avg: 4.11536776845)\n",
      "================================================================================\n",
      "Generation: 6 (with 14632 labeled nodes)\n",
      "Epoch: 1 Loss: 52928.29461 (avg: 3.61729733529)\n",
      "Epoch: 2 Loss: 49720.7153311 (avg: 3.39808059944)\n",
      "Epoch: 3 Loss: 46745.3658886 (avg: 3.19473523022)\n",
      "Epoch: 4 Loss: 44415.3229184 (avg: 3.03549227162)\n",
      "Epoch: 5 Loss: 42669.2744589 (avg: 2.91616145837)\n",
      "Epoch: 6 Loss: 41317.5297766 (avg: 2.82377868894)\n",
      "Epoch: 7 Loss: 40265.9788575 (avg: 2.75191216905)\n",
      "Epoch: 8 Loss: 39481.5065584 (avg: 2.69829869863)\n",
      "Epoch: 9 Loss: 38883.0255482 (avg: 2.65739649728)\n",
      "Epoch: 10 Loss: 38394.5213709 (avg: 2.62401048188)\n",
      "Epoch: 11 Loss: 37997.671833 (avg: 2.59688845223)\n",
      "Epoch: 12 Loss: 37708.6561189 (avg: 2.57713614809)\n",
      "================================================================================\n",
      "Generation: 7 (with 15079 labeled nodes)\n",
      "Epoch: 1 Loss: 38169.4927418 (avg: 2.53130132912)\n",
      "Epoch: 2 Loss: 37938.3335209 (avg: 2.51597145175)\n",
      "Epoch: 3 Loss: 37719.5644593 (avg: 2.50146325747)\n",
      "Epoch: 4 Loss: 37548.3965832 (avg: 2.4901118498)\n",
      "Epoch: 5 Loss: 37342.7619858 (avg: 2.47647469897)\n",
      "Epoch: 6 Loss: 37179.2010965 (avg: 2.46562776686)\n",
      "Epoch: 7 Loss: 37026.7318459 (avg: 2.45551640333)\n",
      "Epoch: 8 Loss: 36869.5685234 (avg: 2.44509374119)\n",
      "Epoch: 9 Loss: 36714.2241678 (avg: 2.43479170819)\n",
      "Epoch: 10 Loss: 36556.062849 (avg: 2.42430286153)\n",
      "Epoch: 11 Loss: 36388.516073 (avg: 2.41319159579)\n",
      "Epoch: 12 Loss: 36279.2999344 (avg: 2.40594866598)\n",
      "================================================================================\n",
      "Generation: 8 (with 15191 labeled nodes)\n",
      "Epoch: 1 Loss: 36292.8302593 (avg: 2.38910080043)\n",
      "Epoch: 2 Loss: 36142.1383142 (avg: 2.3791809831)\n",
      "Epoch: 3 Loss: 35992.7651484 (avg: 2.36934797896)\n",
      "Epoch: 4 Loss: 35817.5787113 (avg: 2.35781572716)\n",
      "Epoch: 5 Loss: 35693.4812863 (avg: 2.34964658589)\n",
      "Epoch: 6 Loss: 35541.9137549 (avg: 2.33966913007)\n",
      "Epoch: 7 Loss: 35338.7059822 (avg: 2.32629227715)\n",
      "Epoch: 8 Loss: 35228.1207852 (avg: 2.31901262493)\n",
      "Epoch: 9 Loss: 35066.7614098 (avg: 2.30839058717)\n",
      "Epoch: 10 Loss: 34929.449475 (avg: 2.2993515552)\n",
      "Epoch: 11 Loss: 34726.7562652 (avg: 2.28600857515)\n",
      "Epoch: 12 Loss: 34534.6163926 (avg: 2.27336030496)\n",
      "================================================================================\n",
      "Generation: 9 (with 15231 labeled nodes)\n",
      "Epoch: 1 Loss: 34397.7548504 (avg: 2.25840423153)\n",
      "Epoch: 2 Loss: 34174.315232 (avg: 2.24373417583)\n",
      "Epoch: 3 Loss: 33952.2677469 (avg: 2.22915552143)\n",
      "Epoch: 4 Loss: 33685.3552818 (avg: 2.21163123116)\n",
      "Epoch: 5 Loss: 33393.6667547 (avg: 2.1924802544)\n",
      "Epoch: 6 Loss: 33087.8357959 (avg: 2.1724007482)\n",
      "Epoch: 7 Loss: 32729.8952062 (avg: 2.14889995445)\n",
      "Epoch: 8 Loss: 32376.446857 (avg: 2.1256941013)\n",
      "Epoch: 9 Loss: 31961.6357994 (avg: 2.09845944451)\n",
      "Epoch: 10 Loss: 31572.7638092 (avg: 2.072927832)\n",
      "Epoch: 11 Loss: 31172.6398132 (avg: 2.04665746262)\n",
      "Epoch: 12 Loss: 30786.9880087 (avg: 2.02133727324)\n",
      "================================================================================\n",
      "Generation: 10 (with 15245 labeled nodes)\n",
      "Epoch: 1 Loss: 30422.3987277 (avg: 1.99556567581)\n",
      "Epoch: 2 Loss: 30031.1926632 (avg: 1.96990440559)\n",
      "Epoch: 3 Loss: 29653.5526869 (avg: 1.94513300669)\n",
      "Epoch: 4 Loss: 29364.4805961 (avg: 1.92617124277)\n",
      "Epoch: 5 Loss: 29107.8818138 (avg: 1.90933957453)\n",
      "Epoch: 6 Loss: 28846.4327445 (avg: 1.89218975038)\n",
      "Epoch: 7 Loss: 28616.3311777 (avg: 1.87709617433)\n",
      "Epoch: 8 Loss: 28405.1213527 (avg: 1.86324180733)\n",
      "Epoch: 9 Loss: 28208.3217132 (avg: 1.85033268043)\n",
      "Epoch: 10 Loss: 28019.0087893 (avg: 1.83791464672)\n",
      "Epoch: 11 Loss: 27816.6929414 (avg: 1.82464368261)\n",
      "Epoch: 12 Loss: 27687.208854 (avg: 1.81615013801)\n",
      "================================================================================\n",
      "Generation: 11 (with 15246 labeled nodes)\n",
      "Epoch: 1 Loss: 27503.8722091 (avg: 1.80400578572)\n",
      "Epoch: 2 Loss: 27358.5666262 (avg: 1.79447505091)\n",
      "Epoch: 3 Loss: 27161.3940016 (avg: 1.78154230628)\n",
      "Epoch: 4 Loss: 27019.7968509 (avg: 1.77225481116)\n",
      "Epoch: 5 Loss: 26896.3928027 (avg: 1.76416061936)\n",
      "Epoch: 6 Loss: 26724.2659686 (avg: 1.75287065254)\n",
      "Epoch: 7 Loss: 26589.8523142 (avg: 1.74405432994)\n",
      "Epoch: 8 Loss: 26446.2132055 (avg: 1.73463290079)\n",
      "Epoch: 9 Loss: 26322.1659445 (avg: 1.72649652004)\n",
      "Epoch: 10 Loss: 26177.7652947 (avg: 1.71702514067)\n",
      "Epoch: 11 Loss: 26049.3370452 (avg: 1.70860140661)\n",
      "Epoch: 12 Loss: 25919.4343473 (avg: 1.70008096204)\n"
     ]
    }
   ],
   "source": [
    "# Important variable from previous cells: node_pairs, node2vec, node2label, seed_nodes, train_node_pairs, test_nodes\n",
    "\n",
    "NGM_model = NGM_FFNN(ALPHA, TF_IDF_DIM, HIDDEN_1_DIM, HIDDEN_2_DIM, NUM_CATEGORIES)\n",
    "start = datetime.now()\n",
    "NGM_model.train_(seed_nodes, train_node_pairs, node2vec, node2label, NUM_EPOCH, BATCH_SIZE, LEARNING_RATE)\n",
    "NGM_time = (datetime.now()-start).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5136.538772\n",
      "8578.217842\n"
     ]
    }
   ],
   "source": [
    "print(baseline_time)\n",
    "print(NGM_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694666666667\n",
      "0.722666666667\n"
     ]
    }
   ],
   "source": [
    "# Important variable from previous cells: node2vec, node2label, test_nodes\n",
    "\n",
    "print(baseline_model.evaluate(test_nodes, node2vec, node2label))\n",
    "print(NGM_model.evaluate(test_nodes, node2vec, node2label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved.\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "baseline_model.save(MODEL_DIR, \"PubMed_baseline\")\n",
    "NGM_model.save(MODEL_DIR, \"PubMed_NGM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
